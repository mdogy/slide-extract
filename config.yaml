# slide-extract Configuration
# 
# This configuration file specifies which LLM provider and model to use
# for analyzing presentation slides and generating speaker notes.
#
# API Keys Setup:
# Store your API keys in ~/.slide_extract_keys.env as environment variables:
# OPENAI_API_KEY=your_openai_key_here
# ANTHROPIC_API_KEY=your_anthropic_key_here
# GOOGLE_AI_API_KEY=your_google_ai_key_here
# OPENROUTER_API_KEY=your_openrouter_key_here

# Active LLM Configuration
# Uncomment one of the provider configurations below

# OpenAI Configuration
#llm:
#  provider: "openai"
#  model: "gpt-4o"
#  max_tokens: 4000
#  temperature: 0.3

# Alternative OpenAI Models (uncomment to use)
# llm:
#   provider: "openai" 
#   model: "gpt-4o-mini"
#   max_tokens: 4000
#   temperature: 0.3

# llm:
#   provider: "openai"
#   model: "gpt-3.5-turbo"
#   max_tokens: 4000
#   temperature: 0.3

# Anthropic Claude Configuration (uncomment to use)
# llm:
#   provider: "anthropic"
#   model: "claude-3-5-sonnet-20241022"
#   max_tokens: 4000
#   temperature: 0.3

# llm:
#   provider: "anthropic"
#   model: "claude-3-haiku-20240307"
#   max_tokens: 4000
#   temperature: 0.3

# Google Gemini Configuration (uncomment to use)
llm:
  provider: "google"
  model: "gemini-2.5-flash"
  max_tokens: 40000
  temperature: 0.3

# llm:
#   provider: "google"
#   model: "gemini-1.5-flash"
#   max_tokens: 4000
#   temperature: 0.3

# OpenRouter Configuration (uncomment to use)
# Provides access to many models through a single API
# llm:
#   provider: "openrouter"
#   model: "anthropic/claude-3-5-sonnet"
#   max_tokens: 4000
#   temperature: 0.3
#   base_url: "https://openrouter.ai/api/v1"

# llm:
#   provider: "openrouter"
#   model: "google/gemini-pro-1.5"
#   max_tokens: 4000
#   temperature: 0.3
#   base_url: "https://openrouter.ai/api/v1"

# llm:
#   provider: "openrouter"
#   model: "openai/gpt-4o"
#   max_tokens: 4000
#   temperature: 0.3
#   base_url: "https://openrouter.ai/api/v1"

# Advanced Configuration Options
processing:
  # Maximum number of slides to process in a single batch
  batch_size: 10
  
  # Timeout for LLM requests (seconds)
  request_timeout: 60
  
  # Number of retry attempts for failed requests
  max_retries: 3
  
  # Enable parallel processing of multiple PDFs
  parallel_processing: true

# Logging Configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "DEBUG"
  
  # Include LLM request/response details in logs (may contain sensitive data)
  log_llm_details: false